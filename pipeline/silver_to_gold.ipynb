{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea604685-55b8-49a9-8db0-5504e207669a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline Silver to Gold - One Big Table (OBT)\n",
    "## Criação de tabelas desnormalizadas para análises avançadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df4297bd-7127-47e8-b973-f43c52a5b0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validando a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919d2553-aba4-437d-8623-d0804f771399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1083472119584360#setting/sparkui/0616-002700-7ayy1x03/driver-5607922679790549446\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1083472119584360#setting/sparkui/0616-002700-7ayy1x03/driver-5607922679790549446\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdacc1be-a39d-43c6-aed9-6ad52835d096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurações iniciais - Azure ADLS Gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b81fc75-2d28-40a8-96bd-50251aa3ad0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storageAccountName = \"datalake7eadf73a479de9f7\"\n",
    "sasToken = \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-06-16T03:33:35Z&st=2025-06-15T19:33:35Z&spr=https&sig=fBJfCWK99G%2FGTBY81Y8eNQYEpOxgrzjVSaavlAB0np4%3D\"\n",
    "\n",
    "def mount_adls(blobContainerName):\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "            source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "            mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n",
    "            extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "        )\n",
    "        print(f\"Container {blobContainerName} montado com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao montar {blobContainerName}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae51aeb-094e-4e40-8881-94b513efafa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Montando containers necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07c6219-02c4-4967-9923-063d21aab317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falha ao montar silver: An error occurred while calling o1915.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/datalake7eadf73a479de9f7/silver; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/datalake7eadf73a479de9f7/silver\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1053)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1079)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1073)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/datalake7eadf73a479de9f7/silver\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:860)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1242)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1015)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1231)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:868)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:131)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:175)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$2(DbfsServerBackend.scala:599)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.com$databricks$backend$daemon$data$server$DbfsServerBackend$$handleOtherRpc(DbfsServerBackend.scala:599)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:524)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:431)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:860)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:860)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:823)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:805)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:291)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:291)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n\nFalha ao montar gold: An error occurred while calling o1915.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/datalake7eadf73a479de9f7/gold; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/datalake7eadf73a479de9f7/gold\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1053)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1079)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1073)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/datalake7eadf73a479de9f7/gold\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:860)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1242)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1015)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1231)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:868)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:131)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:175)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$2(DbfsServerBackend.scala:599)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.com$databricks$backend$daemon$data$server$DbfsServerBackend$$handleOtherRpc(DbfsServerBackend.scala:599)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:524)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:431)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:860)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:860)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:823)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:805)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:291)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:291)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n\n"
     ]
    }
   ],
   "source": [
    "mount_adls('silver')\n",
    "mount_adls('gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c871731-434b-42e2-8d52-dc964e3c5afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importando bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0418e3b-d9cf-4122-ae7a-03b92c4d7987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, lit, col, when, isnan, isnull, \n",
    "    count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    concat_ws, coalesce, date_format, year, month, dayofmonth,\n",
    "    dense_rank, row_number, round as spark_round, collect_list,\n",
    "    current_date, datediff, count_distinct\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84967f45-70c2-404d-b2bd-e18f90d4e824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verificando tabelas disponíveis na camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6404f861-0309-41c8-868f-6dc08412ab95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TABELAS SILVER DISPONÍVEIS ===\n+---------------+--------------------+-----------+\n|       database|           tableName|isTemporary|\n+---------------+--------------------+-----------+\n|pipeline_silver|achievement_unlocked|      false|\n|pipeline_silver|        achievements|      false|\n|pipeline_silver|          developers|      false|\n|pipeline_silver|                dlcs|      false|\n|pipeline_silver|        game_genders|      false|\n|pipeline_silver|      game_platforms|      false|\n|pipeline_silver|           game_tags|      false|\n|pipeline_silver|               games|      false|\n|pipeline_silver|             genders|      false|\n|pipeline_silver|           platforms|      false|\n|pipeline_silver|           purchases|      false|\n|pipeline_silver|             reviews|      false|\n|pipeline_silver|                tags|      false|\n|pipeline_silver|               users|      false|\n+---------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TABELAS SILVER DISPONÍVEIS ===\")\n",
    "spark.sql(\"SHOW TABLES IN pipeline_silver\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "754a25d0-abb2-476a-9b28-93b5f31941c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carregando dados das tabelas Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62bb68a-6723-4a34-9155-a1855690e623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregar_tabelas_silver():\n",
    "    \"\"\"Carrega todas as tabelas da camada Silver\"\"\"\n",
    "    \n",
    "    tabelas = {}\n",
    "    database_name = \"pipeline_silver\"\n",
    "    \n",
    "    try:\n",
    "        # Obter lista de tabelas\n",
    "        tabelas_disponiveis = spark.sql(f\"SHOW TABLES IN {database_name}\").collect()\n",
    "        \n",
    "        for tabela in tabelas_disponiveis:\n",
    "            nome_tabela = tabela['tableName']\n",
    "            print(f\"Carregando tabela: {nome_tabela}\")\n",
    "            \n",
    "            df = spark.sql(f\"SELECT * FROM {database_name}.{nome_tabela}\")\n",
    "            tabelas[nome_tabela] = df\n",
    "            \n",
    "            print(f\"  - Registros: {df.count()}\")\n",
    "            print(f\"  - Colunas: {len(df.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar tabelas: {str(e)}\")\n",
    "    \n",
    "    return tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6c5078-9dda-4b03-b2c2-7b69141637f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tabela: achievement_unlocked\n  - Registros: 2\n  - Colunas: 5\nCarregando tabela: achievements\n  - Registros: 2\n  - Colunas: 6\nCarregando tabela: developers\n  - Registros: 2\n  - Colunas: 4\nCarregando tabela: dlcs\n  - Registros: 2\n  - Colunas: 5\nCarregando tabela: game_genders\n  - Registros: 2\n  - Colunas: 5\nCarregando tabela: game_platforms\n  - Registros: 2\n  - Colunas: 5\nCarregando tabela: game_tags\n  - Registros: 2\n  - Colunas: 5\nCarregando tabela: games\n  - Registros: 2\n  - Colunas: 5\nCarregando tabela: genders\n  - Registros: 2\n  - Colunas: 4\nCarregando tabela: platforms\n  - Registros: 2\n  - Colunas: 4\nCarregando tabela: purchases\n  - Registros: 2\n  - Colunas: 6\nCarregando tabela: reviews\n  - Registros: 2\n  - Colunas: 7\nCarregando tabela: tags\n  - Registros: 2\n  - Colunas: 4\nCarregando tabela: users\n  - Registros: 2\n  - Colunas: 5\n"
     ]
    }
   ],
   "source": [
    "silver_tables = carregar_tabelas_silver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f34c322-66b0-41a2-95b0-2c5b18b00556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Função para salvar OBT na camada Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9251af4-09aa-4059-8425-bd5d0a2cbcfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def salvar_obt_gold(df_obt, nome_tabela):\n",
    "    \"\"\"Salva a OBT na camada Gold\"\"\"\n",
    "    \n",
    "    if df_obt is None:\n",
    "        print(\"DataFrame OBT é None - não é possível salvar\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        gold_path = f\"/mnt/{storageAccountName}/gold\"\n",
    "        \n",
    "        # Salvar como Delta na camada Gold\n",
    "        df_obt.write.format('delta').mode('overwrite').save(f\"{gold_path}/{nome_tabela}\")\n",
    "        print(f\"OBT {nome_tabela} salva na camada Gold\")\n",
    "        \n",
    "        # Criar tabela gerenciada\n",
    "        database_name = \"pipeline_gold\"\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        \n",
    "        df_obt.write.format('delta').mode('overwrite').saveAsTable(f\"{database_name}.{nome_tabela}\")\n",
    "        print(f\"Tabela gerenciada criada: {database_name}.{nome_tabela}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar OBT: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c054cb16-f3fc-4af1-988a-fb59bcb89999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Criando OBT de Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b791e2b-c9e7-45c6-bb9e-66a9f802eb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_games_obt():\n",
    "    \"\"\"Cria OBT focada em games e suas métricas integradas\"\"\"\n",
    "    \n",
    "    try:\n",
    "        games_df = silver_tables.get('games')\n",
    "        if games_df is None:\n",
    "            print(\"Tabela games não encontrada\")\n",
    "            return None\n",
    "            \n",
    "        print(\"=== Criando Games OBT ===\")\n",
    "        \n",
    "        # Join com Developers\n",
    "        developers_df = silver_tables.get('developers')\n",
    "        if developers_df is not None:\n",
    "            games_df = games_df.alias('g').join(\n",
    "                developers_df.alias('d'),\n",
    "                col('g.DEVELOPER_ID') == col('d.ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('g.*'),\n",
    "                col('d.NAME').alias('DEVELOPER_NAME')\n",
    "            )\n",
    "        \n",
    "        # Agregações de Reviews\n",
    "        reviews_df = silver_tables.get('reviews')\n",
    "        if reviews_df is not None:\n",
    "            reviews_agg = reviews_df.groupBy('GAME_ID').agg(\n",
    "                count('*').alias('TOTAL_REVIEWS'),\n",
    "                avg('RATING').alias('AVG_RATING'),\n",
    "                spark_max('RATING').alias('MAX_RATING'),\n",
    "                spark_min('RATING').alias('MIN_RATING')\n",
    "            )\n",
    "            \n",
    "            games_df = games_df.alias('g').join(\n",
    "                reviews_agg.alias('r'),\n",
    "                col('g.ID') == col('r.GAME_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('g.*'),\n",
    "                coalesce(col('r.TOTAL_REVIEWS'), lit(0)).alias('TOTAL_REVIEWS'),\n",
    "                spark_round(coalesce(col('r.AVG_RATING'), lit(0)), 2).alias('AVG_RATING'),\n",
    "                coalesce(col('r.MAX_RATING'), lit(0)).alias('MAX_RATING'),\n",
    "                coalesce(col('r.MIN_RATING'), lit(0)).alias('MIN_RATING')\n",
    "            )\n",
    "        \n",
    "        # Agregações de Purchases\n",
    "        purchases_df = silver_tables.get('purchases')\n",
    "        if purchases_df is not None:\n",
    "            purchases_agg = purchases_df.groupBy('GAME_ID').agg(\n",
    "                count('*').alias('TOTAL_PURCHASES')\n",
    "            )\n",
    "            \n",
    "            games_df = games_df.alias('g').join(\n",
    "                purchases_agg.alias('p'),\n",
    "                col('g.ID') == col('p.GAME_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('g.*'),\n",
    "                coalesce(col('p.TOTAL_PURCHASES'), lit(0)).alias('TOTAL_PURCHASES')\n",
    "            )\n",
    "        \n",
    "        # Agregações de Achievements\n",
    "        achievements_df = silver_tables.get('achievements')\n",
    "        if achievements_df is not None:\n",
    "            achievements_agg = achievements_df.groupBy('GAME_ID').agg(\n",
    "                count('*').alias('TOTAL_ACHIEVEMENTS')\n",
    "            )\n",
    "            \n",
    "            games_df = games_df.alias('g').join(\n",
    "                achievements_agg.alias('a'),\n",
    "                col('g.ID') == col('a.GAME_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('g.*'),\n",
    "                coalesce(col('a.TOTAL_ACHIEVEMENTS'), lit(0)).alias('TOTAL_ACHIEVEMENTS')\n",
    "            )\n",
    "        \n",
    "        # Agregações de DLCs - Apenas contagem (PRICE não disponível na Silver)\n",
    "        dlcs_df = silver_tables.get('dlcs')\n",
    "        if dlcs_df is not None:\n",
    "            dlcs_agg = dlcs_df.groupBy('GAME_ID').agg(\n",
    "                count('*').alias('TOTAL_DLCS')\n",
    "            )\n",
    "            \n",
    "            games_df = games_df.alias('g').join(\n",
    "                dlcs_agg.alias('d'),\n",
    "                col('g.ID') == col('d.GAME_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('g.*'),\n",
    "                coalesce(col('d.TOTAL_DLCS'), lit(0)).alias('TOTAL_DLCS')\n",
    "            )\n",
    "        \n",
    "        # Adicionando campos calculados (removidas colunas não disponíveis)\n",
    "        games_obt = games_df.withColumn(\n",
    "            'RATING_CATEGORY',\n",
    "            when(col('AVG_RATING') >= 9, 'Excelente')\n",
    "            .when(col('AVG_RATING') >= 7, 'Muito Bom')\n",
    "            .when(col('AVG_RATING') >= 5, 'Bom')\n",
    "            .when(col('AVG_RATING') >= 3, 'Regular')\n",
    "            .when(col('AVG_RATING') > 0, 'Ruim')\n",
    "            .otherwise('Sem Avaliação')\n",
    "        ).withColumn(\n",
    "            'POPULARITY_SCORE',\n",
    "            spark_round(\n",
    "                (col('TOTAL_PURCHASES') * 0.5) + \n",
    "                (col('TOTAL_REVIEWS') * 0.3) + \n",
    "                (col('AVG_RATING') * 0.2), 2\n",
    "            )\n",
    "        ).withColumn(\n",
    "            'SUCCESS_TIER',\n",
    "            when(col('TOTAL_PURCHASES') > 1000, 'Blockbuster')\n",
    "            .when(col('TOTAL_PURCHASES') > 500, 'Hit')\n",
    "            .when(col('TOTAL_PURCHASES') > 100, 'Popular')\n",
    "            .when(col('TOTAL_PURCHASES') > 10, 'Moderado')\n",
    "            .when(col('TOTAL_PURCHASES') > 0, 'Nicho')\n",
    "            .otherwise('Sem Vendas')\n",
    "        ).withColumn(\n",
    "            'DATA_HORA_GOLD', current_timestamp()\n",
    "        ).withColumn(\n",
    "            'CAMADA_ORIGEM', lit('SILVER')\n",
    "        )\n",
    "        \n",
    "        print(f\"OBT Games criada com {games_obt.count()} registros\")\n",
    "        print(f\"Total de colunas: {len(games_obt.columns)}\")\n",
    "        \n",
    "        return games_obt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Games OBT: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55bd9461-d47e-4539-899e-687719ea6a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Criando Games OBT ===\nOBT Games criada com 2 registros\nTotal de colunas: 18\nOBT games_obt salva na camada Gold\nTabela gerenciada criada: pipeline_gold.games_obt\n"
     ]
    }
   ],
   "source": [
    "# Executando criação da Games OBT\n",
    "games_obt = criar_games_obt()\n",
    "\n",
    "# Salvando Games OBT\n",
    "salvar_obt_gold(games_obt, 'games_obt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e769823-1c3b-498b-9290-269f8783b8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Criando OBT de Usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68cc8aa-cb70-4451-8ecd-0d06464fabb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_users_obt():\n",
    "    \"\"\"Cria OBT focada em usuários e suas atividades\"\"\"\n",
    "    \n",
    "    try:\n",
    "        users_df = silver_tables.get('users')\n",
    "        if users_df is None:\n",
    "            print(\"Tabela users não encontrada\")\n",
    "            return None\n",
    "            \n",
    "        print(\"=== Criando Users OBT ===\")\n",
    "        \n",
    "        # Agregações de Purchases por usuário (apenas contagem - PAIDPRICE não disponível)\n",
    "        purchases_df = silver_tables.get('purchases')\n",
    "        if purchases_df is not None:\n",
    "            purchases_agg = purchases_df.groupBy('USER_ID').agg(\n",
    "                count('*').alias('TOTAL_GAMES_PURCHASED')\n",
    "            )\n",
    "            \n",
    "            users_df = users_df.alias('u').join(\n",
    "                purchases_agg.alias('p'),\n",
    "                col('u.ID') == col('p.USER_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('u.*'),\n",
    "                coalesce(col('p.TOTAL_GAMES_PURCHASED'), lit(0)).alias('TOTAL_GAMES_PURCHASED')\n",
    "            )\n",
    "        \n",
    "        # Agregações de Reviews por usuário\n",
    "        reviews_df = silver_tables.get('reviews')\n",
    "        if reviews_df is not None:\n",
    "            reviews_agg = reviews_df.groupBy('USER_ID').agg(\n",
    "                count('*').alias('TOTAL_REVIEWS_WRITTEN'),\n",
    "                avg('RATING').alias('AVG_RATING_GIVEN'),\n",
    "                spark_max('RATING').alias('MAX_RATING_GIVEN'),\n",
    "                spark_min('RATING').alias('MIN_RATING_GIVEN')\n",
    "            )\n",
    "            \n",
    "            users_df = users_df.alias('u').join(\n",
    "                reviews_agg.alias('r'),\n",
    "                col('u.ID') == col('r.USER_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('u.*'),\n",
    "                coalesce(col('r.TOTAL_REVIEWS_WRITTEN'), lit(0)).alias('TOTAL_REVIEWS_WRITTEN'),\n",
    "                spark_round(coalesce(col('r.AVG_RATING_GIVEN'), lit(0)), 2).alias('AVG_RATING_GIVEN'),\n",
    "                coalesce(col('r.MAX_RATING_GIVEN'), lit(0)).alias('MAX_RATING_GIVEN'),\n",
    "                coalesce(col('r.MIN_RATING_GIVEN'), lit(0)).alias('MIN_RATING_GIVEN')\n",
    "            )\n",
    "        \n",
    "        # Agregações de Achievements por usuário\n",
    "        achievement_unlocked_df = silver_tables.get('achievement_unlocked')\n",
    "        if achievement_unlocked_df is not None:\n",
    "            achievements_df = silver_tables.get('achievements')\n",
    "            if achievements_df is not None:\n",
    "                # Primeiro verificar se as colunas existem antes de fazer o join\n",
    "                achievements_user_agg = achievement_unlocked_df.alias('au').join(\n",
    "                    achievements_df.alias('a'),\n",
    "                    col('au.ACHIEVEMENT_ID') == col('a.ID'),  # Ajustado para ACHIEVEMENT_ID\n",
    "                    'inner'\n",
    "                ).groupBy('au.USER_ID').agg(  # Ajustado para USER_ID\n",
    "                    count('*').alias('TOTAL_ACHIEVEMENTS_UNLOCKED')\n",
    "                    # Removido POINTS pois pode não existir na Silver\n",
    "                )\n",
    "                \n",
    "                users_df = users_df.alias('u').join(\n",
    "                    achievements_user_agg.alias('au'),\n",
    "                    col('u.ID') == col('au.USER_ID'),\n",
    "                    'left'\n",
    "                ).select(\n",
    "                    col('u.*'),\n",
    "                    coalesce(col('au.TOTAL_ACHIEVEMENTS_UNLOCKED'), lit(0)).alias('TOTAL_ACHIEVEMENTS_UNLOCKED')\n",
    "                )\n",
    "        \n",
    "        # Adicionando campos calculados (removidas colunas não disponíveis)\n",
    "        users_obt = users_df.withColumn(\n",
    "            'ENGAGEMENT_SCORE',\n",
    "            spark_round(\n",
    "                (col('TOTAL_GAMES_PURCHASED') * 2) + \n",
    "                (col('TOTAL_REVIEWS_WRITTEN') * 3) + \n",
    "                (col('TOTAL_ACHIEVEMENTS_UNLOCKED') * 1), 2\n",
    "            )\n",
    "        ).withColumn(\n",
    "            'USER_TYPE',\n",
    "            when((col('TOTAL_REVIEWS_WRITTEN') > 10) & (col('TOTAL_GAMES_PURCHASED') > 5), 'Reviewer Ativo')\n",
    "            .when(col('TOTAL_ACHIEVEMENTS_UNLOCKED') > 50, 'Achievement Hunter')\n",
    "            .when(col('TOTAL_GAMES_PURCHASED') > 20, 'Colecionador')\n",
    "            .otherwise('Casual')\n",
    "        ).withColumn(\n",
    "            'ACTIVITY_LEVEL',\n",
    "            when(col('ENGAGEMENT_SCORE') > 100, 'Muito Ativo')\n",
    "            .when(col('ENGAGEMENT_SCORE') > 50, 'Ativo')\n",
    "            .when(col('ENGAGEMENT_SCORE') > 20, 'Moderado')\n",
    "            .when(col('ENGAGEMENT_SCORE') > 0, 'Baixo')\n",
    "            .otherwise('Inativo')\n",
    "        ).withColumn(\n",
    "            'DATA_HORA_GOLD', current_timestamp()\n",
    "        ).withColumn(\n",
    "            'CAMADA_ORIGEM', lit('SILVER')\n",
    "        )\n",
    "        \n",
    "        print(f\"OBT Users criada com {users_obt.count()} registros\")\n",
    "        print(f\"Total de colunas: {len(users_obt.columns)}\")\n",
    "        \n",
    "        return users_obt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Users OBT: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae75574e-51af-4cf1-ab4a-50ff7bc58c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Criando Users OBT ===\nOBT Users criada com 2 registros\nTotal de colunas: 16\nOBT users_obt salva na camada Gold\nTabela gerenciada criada: pipeline_gold.users_obt\n"
     ]
    }
   ],
   "source": [
    "# Executando criação da Users OBT\n",
    "users_obt = criar_users_obt()\n",
    "\n",
    "# Salvando Users OBT\n",
    "salvar_obt_gold(users_obt, 'users_obt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c41d0b-7a9a-42fd-897c-5b1a5824d26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Criando OBT de Desenvolvedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc62c931-b133-4587-bb16-f28ce755f7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_developers_obt():\n",
    "    \"\"\"Cria OBT focada em desenvolvedores e performance de seus jogos\"\"\"\n",
    "    \n",
    "    try:\n",
    "        developers_df = silver_tables.get('developers')\n",
    "        if developers_df is None:\n",
    "            print(\"Tabela developers não encontrada\")\n",
    "            return None\n",
    "            \n",
    "        print(\"=== Criando Developers OBT ===\")\n",
    "        \n",
    "        # Agregações de Games por desenvolvedor\n",
    "        games_df = silver_tables.get('games')\n",
    "        if games_df is not None:\n",
    "            # CORREÇÃO: DEVELOPERID -> DEVELOPER_ID\n",
    "            games_agg = games_df.groupBy('DEVELOPER_ID').agg(\n",
    "                count('*').alias('TOTAL_GAMES_DEVELOPED'),\n",
    "                # Removidas as colunas de PRICE que não existem na Silver\n",
    "                # avg('PRICE').alias('AVG_GAME_PRICE'),\n",
    "                # spark_max('PRICE').alias('MAX_GAME_PRICE'),\n",
    "                # spark_min('PRICE').alias('MIN_GAME_PRICE'),\n",
    "                # spark_sum('PRICE').alias('TOTAL_GAMES_VALUE')\n",
    "            )\n",
    "            \n",
    "            developers_df = developers_df.alias('d').join(\n",
    "                games_agg.alias('g'),\n",
    "                col('d.ID') == col('g.DEVELOPER_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('d.*'),\n",
    "                coalesce(col('g.TOTAL_GAMES_DEVELOPED'), lit(0)).alias('TOTAL_GAMES_DEVELOPED')\n",
    "                # Removidas as colunas de preço\n",
    "            )\n",
    "        \n",
    "        # Agregações de Purchases por desenvolvedor (via games)\n",
    "        purchases_df = silver_tables.get('purchases')\n",
    "        if purchases_df is not None and games_df is not None:\n",
    "            purchases_dev_agg = purchases_df.alias('p').join(\n",
    "                games_df.alias('g'),\n",
    "                col('p.GAME_ID') == col('g.ID'),  # CORREÇÃO: GAMEID -> GAME_ID\n",
    "                'inner'\n",
    "            ).groupBy('g.DEVELOPER_ID').agg(\n",
    "                count('*').alias('TOTAL_SALES')\n",
    "                # Removidas as colunas de PAIDPRICE que não existem na Silver\n",
    "                # spark_sum('p.PAIDPRICE').alias('TOTAL_REVENUE'),\n",
    "                # avg('p.PAIDPRICE').alias('AVG_SALE_PRICE')\n",
    "            )\n",
    "            \n",
    "            developers_df = developers_df.alias('d').join(\n",
    "                purchases_dev_agg.alias('p'),\n",
    "                col('d.ID') == col('p.DEVELOPER_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('d.*'),\n",
    "                coalesce(col('p.TOTAL_SALES'), lit(0)).alias('TOTAL_SALES')\n",
    "                # Removidas as colunas de revenue\n",
    "            )\n",
    "        \n",
    "        # Agregações de Reviews por desenvolvedor (via games)\n",
    "        reviews_df = silver_tables.get('reviews')\n",
    "        if reviews_df is not None and games_df is not None:\n",
    "            reviews_dev_agg = reviews_df.alias('r').join(\n",
    "                games_df.alias('g'),\n",
    "                col('r.GAME_ID') == col('g.ID'),  # CORREÇÃO: GAMEID -> GAME_ID\n",
    "                'inner'\n",
    "            ).groupBy('g.DEVELOPER_ID').agg(\n",
    "                count('*').alias('TOTAL_REVIEWS_RECEIVED'),\n",
    "                avg('r.RATING').alias('AVG_RATING_RECEIVED'),\n",
    "                spark_max('r.RATING').alias('MAX_RATING_RECEIVED'),\n",
    "                spark_min('r.RATING').alias('MIN_RATING_RECEIVED')\n",
    "            )\n",
    "            \n",
    "            developers_df = developers_df.alias('d').join(\n",
    "                reviews_dev_agg.alias('r'),\n",
    "                col('d.ID') == col('r.DEVELOPER_ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('d.*'),\n",
    "                coalesce(col('r.TOTAL_REVIEWS_RECEIVED'), lit(0)).alias('TOTAL_REVIEWS_RECEIVED'),\n",
    "                spark_round(coalesce(col('r.AVG_RATING_RECEIVED'), lit(0)), 2).alias('AVG_RATING_RECEIVED'),\n",
    "                coalesce(col('r.MAX_RATING_RECEIVED'), lit(0)).alias('MAX_RATING_RECEIVED'),\n",
    "                coalesce(col('r.MIN_RATING_RECEIVED'), lit(0)).alias('MIN_RATING_RECEIVED')\n",
    "            )\n",
    "        \n",
    "        # Adicionando campos calculados (ajustados para colunas disponíveis)\n",
    "        developers_obt = developers_df.withColumn(\n",
    "            'SALES_PER_GAME',\n",
    "            when(col('TOTAL_GAMES_DEVELOPED') > 0,\n",
    "                 spark_round(col('TOTAL_SALES') / col('TOTAL_GAMES_DEVELOPED'), 2)\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\n",
    "            'REVIEWS_PER_GAME',\n",
    "            when(col('TOTAL_GAMES_DEVELOPED') > 0,\n",
    "                 spark_round(col('TOTAL_REVIEWS_RECEIVED') / col('TOTAL_GAMES_DEVELOPED'), 2)\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\n",
    "            'DEVELOPER_TIER',\n",
    "            when(col('TOTAL_SALES') > 1000, 'AAA')\n",
    "            .when(col('TOTAL_SALES') > 500, 'AA')\n",
    "            .when(col('TOTAL_SALES') > 100, 'Indie Premium')\n",
    "            .when(col('TOTAL_SALES') > 0, 'Indie')\n",
    "            .otherwise('Sem Vendas')\n",
    "        ).withColumn(\n",
    "            'SUCCESS_SCORE',\n",
    "            spark_round(\n",
    "                (col('TOTAL_SALES') * 0.5) + \n",
    "                (col('AVG_RATING_RECEIVED') * 100 * 0.3) + \n",
    "                (col('TOTAL_GAMES_DEVELOPED') * 10 * 0.2), 2\n",
    "            )\n",
    "        ).withColumn(\n",
    "            'QUALITY_SCORE',\n",
    "            when(col('TOTAL_REVIEWS_RECEIVED') > 0,\n",
    "                 spark_round(col('AVG_RATING_RECEIVED') * 20, 2)\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\n",
    "            'DATA_HORA_GOLD', current_timestamp()\n",
    "        ).withColumn(\n",
    "            'CAMADA_ORIGEM', lit('SILVER')\n",
    "        )\n",
    "        \n",
    "        print(f\"OBT Developers criada com {developers_obt.count()} registros\")\n",
    "        print(f\"Total de colunas: {len(developers_obt.columns)}\")\n",
    "        \n",
    "        return developers_obt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Developers OBT: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4a8939-d3db-49f3-a9d3-251cc184c226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Criando Developers OBT ===\nOBT Developers criada com 2 registros\nTotal de colunas: 17\nOBT developers_obt salva na camada Gold\nTabela gerenciada criada: pipeline_gold.developers_obt\n"
     ]
    }
   ],
   "source": [
    "# Executando criação da Developers OBT\n",
    "developers_obt = criar_developers_obt()\n",
    "\n",
    "# Salvando Developers OBT\n",
    "salvar_obt_gold(developers_obt, 'developers_obt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d46f9c-bd38-49ce-bab9-db9367f070aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Criando OBT de Vendas e Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5d26da-c810-4630-8390-07bf9af37073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports necessários (adicione no início do seu arquivo)\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, year, month, dayofmonth, date_format, \n",
    "    current_date, current_timestamp, datediff, count, \n",
    "    count_distinct, min, max, avg, coalesce, round as spark_round\n",
    ")\n",
    "\n",
    "def criar_sales_performance_obt():\n",
    "    \"\"\"Cria OBT focada em análise de vendas e performance\"\"\"\n",
    "    \n",
    "    try:\n",
    "        purchases_df = silver_tables.get('purchases')\n",
    "        if purchases_df is None:\n",
    "            print(\"Tabela purchases não encontrada\")\n",
    "            return None\n",
    "            \n",
    "        print(\"=== Criando Sales Performance OBT ===\")\n",
    "        \n",
    "        # Join com Games\n",
    "        games_df = silver_tables.get('games')\n",
    "        if games_df is not None:\n",
    "            # CORREÇÃO: GAMEID -> GAME_ID\n",
    "            sales_df = purchases_df.alias('p').join(\n",
    "                games_df.alias('g'),\n",
    "                col('p.GAME_ID') == col('g.ID'),\n",
    "                'inner'\n",
    "            ).select(\n",
    "                col('p.*'),\n",
    "                col('g.TITLE').alias('GAME_NAME'),  # CORREÇÃO: NAME -> TITLE (conforme schema)\n",
    "                col('g.DEVELOPER_ID')\n",
    "                # Removidas colunas que não existem: PRICE, RELEASEDATE\n",
    "            )\n",
    "        else:\n",
    "            sales_df = purchases_df\n",
    "        \n",
    "        # Join com Users\n",
    "        users_df = silver_tables.get('users')\n",
    "        if users_df is not None:\n",
    "            # CORREÇÃO: USERID -> USER_ID\n",
    "            sales_df = sales_df.alias('s').join(\n",
    "                users_df.alias('u'),\n",
    "                col('s.USER_ID') == col('u.ID'),\n",
    "                'left'\n",
    "            ).select(\n",
    "                col('s.*'),\n",
    "                # Removidas colunas que podem não existir: COUNTRY, REGISTRATIONDATE\n",
    "                # Mantendo apenas o que sabemos que existe\n",
    "            )\n",
    "        \n",
    "        # Join com Developers\n",
    "        developers_df = silver_tables.get('developers')\n",
    "        if developers_df is not None:\n",
    "            # CORREÇÃO: Verificação correta das colunas\n",
    "            column_names = sales_df.columns\n",
    "            if 'DEVELOPER_ID' in column_names:\n",
    "                sales_df = sales_df.alias('s').join(\n",
    "                    developers_df.alias('d'),\n",
    "                    col('s.DEVELOPER_ID') == col('d.ID'),\n",
    "                    'left'\n",
    "                ).select(\n",
    "                    col('s.*'),\n",
    "                    col('d.NAME').alias('DEVELOPER_NAME')\n",
    "                    # Removida coluna que pode não existir: COUNTRY\n",
    "                )\n",
    "        \n",
    "        # Adicionando campos calculados (apenas com colunas que existem)\n",
    "        sales_obt = sales_df.withColumn(\n",
    "            # CORREÇÃO: PURCHASEDATE -> PURCHASE_DATE\n",
    "            'PURCHASE_YEAR', year(col('PURCHASE_DATE'))\n",
    "        ).withColumn(\n",
    "            'PURCHASE_MONTH', month(col('PURCHASE_DATE'))\n",
    "        ).withColumn(\n",
    "            'PURCHASE_DAY', dayofmonth(col('PURCHASE_DATE'))\n",
    "        ).withColumn(\n",
    "            'PURCHASE_QUARTER', \n",
    "            when(month(col('PURCHASE_DATE')).between(1, 3), 'Q1')\n",
    "            .when(month(col('PURCHASE_DATE')).between(4, 6), 'Q2')\n",
    "            .when(month(col('PURCHASE_DATE')).between(7, 9), 'Q3')\n",
    "            .otherwise('Q4')\n",
    "        ).withColumn(\n",
    "            'PURCHASE_WEEKDAY',\n",
    "            date_format(col('PURCHASE_DATE'), 'EEEE')\n",
    "        ).withColumn(\n",
    "            'PURCHASE_MONTH_NAME',\n",
    "            date_format(col('PURCHASE_DATE'), 'MMMM')\n",
    "        ).withColumn(\n",
    "            'DAYS_SINCE_PURCHASE',\n",
    "            datediff(current_date(), col('PURCHASE_DATE'))\n",
    "        ).withColumn(\n",
    "            'IS_RECENT_PURCHASE',\n",
    "            when(datediff(current_date(), col('PURCHASE_DATE')) <= 30, 'Sim').otherwise('Não')\n",
    "        ).withColumn(\n",
    "            'PURCHASE_SEASON',\n",
    "            when(month(col('PURCHASE_DATE')).isin([12, 1, 2]), 'Verão')\n",
    "            .when(month(col('PURCHASE_DATE')).isin([3, 4, 5]), 'Outono')\n",
    "            .when(month(col('PURCHASE_DATE')).isin([6, 7, 8]), 'Inverno')\n",
    "            .otherwise('Primavera')\n",
    "        ).withColumn(\n",
    "            'DATA_HORA_GOLD', current_timestamp()\n",
    "        ).withColumn(\n",
    "            'CAMADA_ORIGEM', lit('SILVER')\n",
    "        )\n",
    "        \n",
    "        print(f\"OBT Sales Performance criada com {sales_obt.count()} registros\")\n",
    "        print(f\"Total de colunas: {len(sales_obt.columns)}\")\n",
    "        \n",
    "        return sales_obt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Sales Performance OBT: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def criar_user_behavior_obt():\n",
    "    \"\"\"Cria OBT focada em comportamento e análise de usuários\"\"\"\n",
    "    \n",
    "    try:\n",
    "        users_df = silver_tables.get('users')\n",
    "        if users_df is None:\n",
    "            print(\"Tabela users não encontrada\")\n",
    "            return None\n",
    "            \n",
    "        print(\"=== Criando User Behavior OBT ===\")\n",
    "        \n",
    "        # Inicializar DataFrame base\n",
    "        users_base_df = users_df\n",
    "        \n",
    "        # Agregações de Purchases por usuário\n",
    "        purchases_df = silver_tables.get('purchases')\n",
    "        if purchases_df is not None:\n",
    "            # Agregação temporal de compras\n",
    "            purchases_behavior = purchases_df.withColumn(\n",
    "                'PURCHASE_YEAR', year(col('PURCHASE_DATE'))\n",
    "            ).withColumn(\n",
    "                'PURCHASE_MONTH', month(col('PURCHASE_DATE'))\n",
    "            ).groupBy('USER_ID', 'PURCHASE_YEAR', 'PURCHASE_MONTH').agg(\n",
    "                count('*').alias('MONTHLY_PURCHASES')\n",
    "            )\n",
    "            \n",
    "            # Agregação geral por usuário\n",
    "            purchases_agg = purchases_df.groupBy('USER_ID').agg(\n",
    "                count('*').alias('TOTAL_PURCHASES'),\n",
    "                count_distinct('GAME_ID').alias('UNIQUE_GAMES_PURCHASED'),\n",
    "                min('PURCHASE_DATE').alias('FIRST_PURCHASE_DATE'),\n",
    "                max('PURCHASE_DATE').alias('LAST_PURCHASE_DATE')\n",
    "            )\n",
    "            \n",
    "            # Join com purchases agregado\n",
    "            users_base_df = users_base_df.join(\n",
    "                purchases_agg,\n",
    "                users_base_df.ID == purchases_agg.USER_ID,\n",
    "                'left'\n",
    "            ).select(\n",
    "                users_base_df['*'],\n",
    "                coalesce(purchases_agg.TOTAL_PURCHASES, lit(0)).alias('TOTAL_PURCHASES'),\n",
    "                coalesce(purchases_agg.UNIQUE_GAMES_PURCHASED, lit(0)).alias('UNIQUE_GAMES_PURCHASED'),\n",
    "                purchases_agg.FIRST_PURCHASE_DATE,\n",
    "                purchases_agg.LAST_PURCHASE_DATE\n",
    "            )\n",
    "        else:\n",
    "            # Se não há purchases, adicionar colunas com valores default\n",
    "            users_base_df = users_base_df.withColumn('TOTAL_PURCHASES', lit(0)) \\\n",
    "                                       .withColumn('UNIQUE_GAMES_PURCHASED', lit(0)) \\\n",
    "                                       .withColumn('FIRST_PURCHASE_DATE', lit(None)) \\\n",
    "                                       .withColumn('LAST_PURCHASE_DATE', lit(None))\n",
    "        \n",
    "        # Agregações de Reviews por usuário\n",
    "        reviews_df = silver_tables.get('reviews')\n",
    "        if reviews_df is not None:\n",
    "            reviews_agg = reviews_df.groupBy('USER_ID').agg(\n",
    "                count('*').alias('TOTAL_REVIEWS'),\n",
    "                avg('RATING').alias('AVG_RATING_GIVEN'),\n",
    "                count_distinct('GAME_ID').alias('UNIQUE_GAMES_REVIEWED'),\n",
    "                min('RATING').alias('MIN_RATING_GIVEN'),\n",
    "                max('RATING').alias('MAX_RATING_GIVEN')\n",
    "            )\n",
    "            \n",
    "            # Join com reviews agregado\n",
    "            users_base_df = users_base_df.join(\n",
    "                reviews_agg,\n",
    "                users_base_df.ID == reviews_agg.USER_ID,\n",
    "                'left'\n",
    "            ).select(\n",
    "                users_base_df['*'],\n",
    "                coalesce(reviews_agg.TOTAL_REVIEWS, lit(0)).alias('TOTAL_REVIEWS'),\n",
    "                spark_round(coalesce(reviews_agg.AVG_RATING_GIVEN, lit(0)), 2).alias('AVG_RATING_GIVEN'),\n",
    "                coalesce(reviews_agg.UNIQUE_GAMES_REVIEWED, lit(0)).alias('UNIQUE_GAMES_REVIEWED'),\n",
    "                coalesce(reviews_agg.MIN_RATING_GIVEN, lit(0)).alias('MIN_RATING_GIVEN'),\n",
    "                coalesce(reviews_agg.MAX_RATING_GIVEN, lit(0)).alias('MAX_RATING_GIVEN')\n",
    "            )\n",
    "        else:\n",
    "            # Se não há reviews, adicionar colunas com valores default\n",
    "            users_base_df = users_base_df.withColumn('TOTAL_REVIEWS', lit(0)) \\\n",
    "                                       .withColumn('AVG_RATING_GIVEN', lit(0)) \\\n",
    "                                       .withColumn('UNIQUE_GAMES_REVIEWED', lit(0)) \\\n",
    "                                       .withColumn('MIN_RATING_GIVEN', lit(0)) \\\n",
    "                                       .withColumn('MAX_RATING_GIVEN', lit(0))\n",
    "        \n",
    "        # Agregações de Achievements\n",
    "        achievement_unlocked_df = silver_tables.get('achievement_unlocked')\n",
    "        if achievement_unlocked_df is not None:\n",
    "            achievements_agg = achievement_unlocked_df.groupBy('USER_ID').agg(\n",
    "                count('*').alias('TOTAL_ACHIEVEMENTS'),\n",
    "                count_distinct('ACHIEVEMENT_ID').alias('UNIQUE_ACHIEVEMENTS')\n",
    "            )\n",
    "            \n",
    "            # Join com achievements agregado\n",
    "            users_base_df = users_base_df.join(\n",
    "                achievements_agg,\n",
    "                users_base_df.ID == achievements_agg.USER_ID,\n",
    "                'left'\n",
    "            ).select(\n",
    "                users_base_df['*'],\n",
    "                coalesce(achievements_agg.TOTAL_ACHIEVEMENTS, lit(0)).alias('TOTAL_ACHIEVEMENTS'),\n",
    "                coalesce(achievements_agg.UNIQUE_ACHIEVEMENTS, lit(0)).alias('UNIQUE_ACHIEVEMENTS')\n",
    "            )\n",
    "        else:\n",
    "            # Se não há achievements, adicionar colunas com valores default\n",
    "            users_base_df = users_base_df.withColumn('TOTAL_ACHIEVEMENTS', lit(0)) \\\n",
    "                                       .withColumn('UNIQUE_ACHIEVEMENTS', lit(0))\n",
    "        \n",
    "        # Adicionando campos calculados de comportamento\n",
    "        user_behavior_obt = users_base_df.withColumn(\n",
    "            'PURCHASE_FREQUENCY',\n",
    "            when(col('TOTAL_PURCHASES') > 50, 'Muito Alta')\n",
    "            .when(col('TOTAL_PURCHASES') > 20, 'Alta')\n",
    "            .when(col('TOTAL_PURCHASES') > 10, 'Média')\n",
    "            .when(col('TOTAL_PURCHASES') > 0, 'Baixa')\n",
    "            .otherwise('Nenhuma')\n",
    "        ).withColumn(\n",
    "            'REVIEW_ENGAGEMENT',\n",
    "            when(col('TOTAL_REVIEWS') > 20, 'Muito Engajado')\n",
    "            .when(col('TOTAL_REVIEWS') > 10, 'Engajado')\n",
    "            .when(col('TOTAL_REVIEWS') > 5, 'Moderado')\n",
    "            .when(col('TOTAL_REVIEWS') > 0, 'Baixo')\n",
    "            .otherwise('Não Avalia')\n",
    "        ).withColumn(\n",
    "            'ACHIEVEMENT_HUNTER_LEVEL',\n",
    "            when(col('TOTAL_ACHIEVEMENTS') > 100, 'Expert')\n",
    "            .when(col('TOTAL_ACHIEVEMENTS') > 50, 'Avançado')\n",
    "            .when(col('TOTAL_ACHIEVEMENTS') > 20, 'Intermediário')\n",
    "            .when(col('TOTAL_ACHIEVEMENTS') > 0, 'Iniciante')\n",
    "            .otherwise('Não Coleciona')\n",
    "        ).withColumn(\n",
    "            'USER_PROFILE',\n",
    "            when((col('TOTAL_PURCHASES') > 20) & (col('TOTAL_REVIEWS') > 10), 'Comprador e Revisor')\n",
    "            .when((col('TOTAL_PURCHASES') > 20) & (col('TOTAL_ACHIEVEMENTS') > 50), 'Comprador e Colecionador')\n",
    "            .when((col('TOTAL_REVIEWS') > 15) & (col('TOTAL_ACHIEVEMENTS') > 30), 'Revisor e Colecionador')\n",
    "            .when(col('TOTAL_PURCHASES') > 30, 'Grande Comprador')\n",
    "            .when(col('TOTAL_REVIEWS') > 20, 'Grande Revisor')\n",
    "            .when(col('TOTAL_ACHIEVEMENTS') > 80, 'Grande Colecionador')\n",
    "            .otherwise('Casual')\n",
    "        ).withColumn(\n",
    "            'RATING_TENDENCY',\n",
    "            when(col('AVG_RATING_GIVEN') >= 8, 'Positivo')\n",
    "            .when(col('AVG_RATING_GIVEN') >= 6, 'Neutro')\n",
    "            .when(col('AVG_RATING_GIVEN') > 0, 'Crítico')\n",
    "            .otherwise('Não Classificado')\n",
    "        ).withColumn(\n",
    "            'DAYS_SINCE_LAST_PURCHASE',\n",
    "            when(col('LAST_PURCHASE_DATE').isNotNull(),\n",
    "                 datediff(current_date(), col('LAST_PURCHASE_DATE'))\n",
    "            ).otherwise(None)\n",
    "        ).withColumn(\n",
    "            'CUSTOMER_LIFETIME_DAYS',\n",
    "            when((col('FIRST_PURCHASE_DATE').isNotNull()) & (col('LAST_PURCHASE_DATE').isNotNull()),\n",
    "                 datediff(col('LAST_PURCHASE_DATE'), col('FIRST_PURCHASE_DATE'))\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\n",
    "            'CUSTOMER_STATUS',\n",
    "            when(col('DAYS_SINCE_LAST_PURCHASE') <= 30, 'Ativo')\n",
    "            .when(col('DAYS_SINCE_LAST_PURCHASE') <= 90, 'Em Risco')\n",
    "            .when(col('DAYS_SINCE_LAST_PURCHASE') <= 180, 'Inativo')\n",
    "            .when(col('DAYS_SINCE_LAST_PURCHASE').isNotNull(), 'Perdido')\n",
    "            .otherwise('Sem Compras')\n",
    "        ).withColumn(\n",
    "            'DATA_HORA_GOLD', current_timestamp()\n",
    "        ).withColumn(\n",
    "            'CAMADA_ORIGEM', lit('SILVER')\n",
    "        )\n",
    "        \n",
    "        print(f\"OBT User Behavior criada com {user_behavior_obt.count()} registros\")\n",
    "        print(f\"Total de colunas: {len(user_behavior_obt.columns)}\")\n",
    "        \n",
    "        return user_behavior_obt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar User Behavior OBT: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26db4bf3-d193-4d22-a3a2-bb2066f9c717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Criando Sales Performance OBT ===\nOBT Sales Performance criada com 2 registros\nTotal de colunas: 20\nOBT sales_performance_obt salva na camada Gold\nTabela gerenciada criada: pipeline_gold.sales_performance_obt\n=== Criando User Behavior OBT ===\nOBT User Behavior criada com 2 registros\nTotal de colunas: 26\nOBT user_behavior_obt salva na camada Gold\nTabela gerenciada criada: pipeline_gold.user_behavior_obt\n"
     ]
    }
   ],
   "source": [
    "# Sales Performance OBT\n",
    "sales_obt = criar_sales_performance_obt()\n",
    "salvar_obt_gold(sales_obt, 'sales_performance_obt')\n",
    "\n",
    "# User Behavior OBT (nova)\n",
    "user_behavior_obt = criar_user_behavior_obt()\n",
    "salvar_obt_gold(user_behavior_obt, 'user_behavior_obt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5593abde-3c9b-4557-8340-b53b273880d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Resumo das OBTs Criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f41aa6-d99b-493b-989d-51a9274b6cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== RESUMO DAS OBTs CRIADAS ===\n✓ Games OBT: 2 registros, 18 colunas\n✓ Users OBT: 2 registros, 16 colunas\n✓ Developers OBT: 2 registros, 17 colunas\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RESUMO DAS OBTs CRIADAS ===\")\n",
    "\n",
    "if games_obt is not None:\n",
    "    print(f\"✓ Games OBT: {games_obt.count()} registros, {len(games_obt.columns)} colunas\")\n",
    "\n",
    "if users_obt is not None:\n",
    "    print(f\"✓ Users OBT: {users_obt.count()} registros, {len(users_obt.columns)} colunas\")\n",
    "\n",
    "if developers_obt is not None:\n",
    "    print(f\"✓ Developers OBT: {developers_obt.count()} registros, {len(developers_obt.columns)} colunas\")\n",
    "\n",
    "if sales_performance_obt is not None:\n",
    "    print(f\"✓ Sales Performance OBT: {sales_performance_obt.count()} registros, {len(sales_performance_obt.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832e168f-1ef5-4b4a-aba5-4d8e14142680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verificando Tabelas Gold Criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99189284-c9e3-4455-b177-7dd6c44ae2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== TABELAS GOLD DISPONÍVEIS ===\n+-------------+--------------------+-----------+\n|     database|           tableName|isTemporary|\n+-------------+--------------------+-----------+\n|pipeline_gold|      developers_obt|      false|\n|pipeline_gold|           games_obt|      false|\n|pipeline_gold|sales_performance...|      false|\n|pipeline_gold|   user_behavior_obt|      false|\n|pipeline_gold|           users_obt|      false|\n+-------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TABELAS GOLD DISPONÍVEIS ===\")\n",
    "try:\n",
    "    spark.sql(\"SHOW TABLES IN pipeline_gold\").show()\n",
    "except:\n",
    "    print(\"Database pipeline_gold ainda não foi criado ou não contém tabelas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "257c780f-7ecb-42d7-9862-13f45e909115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exemplos de Análises com as OBTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8ae96fe-3a31-49b8-89ca-6667ee29fef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Análise 1: Top 10 Jogos Mais Vendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf139e5-eb61-4af6-bf6b-c3dfaa5d3710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== TOP 10 JOGOS MAIS VENDIDOS ===\n+--------------------+---------------+----------+----------------+\n|               TITLE|TOTAL_PURCHASES|AVG_RATING|  DEVELOPER_NAME|\n+--------------------+---------------+----------+----------------+\n|Elder Scrolls V: ...|              1|       9.0|        Bethesda|\n|               Hades|              1|       8.0|Supergiant Games|\n+--------------------+---------------+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TOP 10 JOGOS MAIS VENDIDOS ===\")\n",
    "if games_obt is not None:\n",
    "    games_obt.select('TITLE', 'TOTAL_PURCHASES', 'AVG_RATING', 'DEVELOPER_NAME') \\\n",
    "        .orderBy(col('TOTAL_PURCHASES').desc()) \\\n",
    "        .limit(10) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47e52ed8-0d7d-4fe3-bf1e-5c07d66ec80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Análise 2: Desenvolvedores com Melhor Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9191bc5a-64c8-4ae2-a0f4-c451c95d634b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== TOP 10 DESENVOLVEDORES POR VENDAS ===\n+----------------+---------------------+-----------+-------------------+--------------+\n|NAME            |TOTAL_GAMES_DEVELOPED|TOTAL_SALES|AVG_RATING_RECEIVED|DEVELOPER_TIER|\n+----------------+---------------------+-----------+-------------------+--------------+\n|Bethesda        |1                    |1          |9.0                |Indie         |\n|Supergiant Games|1                    |1          |8.0                |Indie         |\n+----------------+---------------------+-----------+-------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TOP 10 DESENVOLVEDORES POR VENDAS ===\")\n",
    "if developers_obt is not None:\n",
    "    developers_obt.select('NAME', 'TOTAL_GAMES_DEVELOPED', 'TOTAL_SALES', 'AVG_RATING_RECEIVED', 'DEVELOPER_TIER') \\\n",
    "        .orderBy(col('TOTAL_SALES').desc()) \\\n",
    "        .limit(10) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "927b0826-3ffa-43c9-8824-3bfc605c8a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Análise 3: Usuários Mais Engajados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f0c3e8-0268-485f-bb8f-c69a857c0d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== TOP 10 USUÁRIOS MAIS ENGAJADOS ===\n+-----+-----------------+---------------------+---------------------+----------------+---------+\n|NAME |EMAIL            |TOTAL_GAMES_PURCHASED|TOTAL_REVIEWS_WRITTEN|ENGAGEMENT_SCORE|USER_TYPE|\n+-----+-----------------+---------------------+---------------------+----------------+---------+\n|Alice|alice@example.com|1                    |1                    |6               |Casual   |\n|Bob  |bob@example.com  |1                    |1                    |6               |Casual   |\n+-----+-----------------+---------------------+---------------------+----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TOP 10 USUÁRIOS MAIS ENGAJADOS ===\")\n",
    "if users_obt is not None:\n",
    "    users_obt.select('NAME', 'EMAIL', 'TOTAL_GAMES_PURCHASED', 'TOTAL_REVIEWS_WRITTEN', 'ENGAGEMENT_SCORE', 'USER_TYPE') \\\n",
    "        .orderBy(col('ENGAGEMENT_SCORE').desc()) \\\n",
    "        .limit(10) \\\n",
    "        .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_to_gold",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}